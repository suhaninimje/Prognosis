# -*- coding: utf-8 -*-
"""Deep_learning_for_Vector_Borne_Diseases.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LimnU3LWPV0i3aB9TCX4tPUbK2rRqYvq
"""

# Update these paths according to your Drive structure:
img_folder = '/home/asareen/shared/Prognosis/Kolhapur_Sentinel_Images/images'  # This is your dataset_rbg folder
# If you use annotation JSONs later, you might set:

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import math
import os
import time
import sys
sys.path.insert(0,'..')

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import  mean_absolute_error
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.metrics import r2_score
import seaborn as sns
from scipy import signal
import pickle

from sklearn.decomposition import PCA

from epiweeks import Week, Year
from datetime import date

from random import randint, randrange
from skimage import io
from skimage.transform import rescale, resize, downscale_local_mean

import skimage
import cv2
import os
import pandas as pd
import numpy as np
import pickle
import plotly.express as px
import matplotlib.pyplot as plt


import os
import json
import pandas as pd
from datetime import date
from epiweeks import Week
from skimage import io
from skimage.transform import rescale

def getEpiWeek(origin_str):
    # Expecting origin_str in the format "YYYY-MM-DD"
    date_ls = origin_str.split('-')
    if len(date_ls) < 3:
        raise ValueError("Invalid date string (expected format YYYY-MM-DD): " + origin_str)
    return Week.fromdate(date(int(date_ls[0]), int(date_ls[1]), int(date_ls[2])))

def readImg(img_path, resize_ratio=None):
    # Read the image and rescale if a resize_ratio is provided
    img = io.imread(img_path)
    if resize_ratio:
        img_rescale = rescale(img, resize_ratio, anti_aliasing=True)
    else:
        img_rescale = img
    print(os.path.basename(img_path), "(origin shape:", img.shape, "-> rescale:", str(img_rescale.shape) + ")")
    return img_rescale

def create_csv_from_jsons(annotations_folder, output_csv):
    """
    Reads all JSON files from the specified annotations folder,
    extracts key information, and writes a combined CSV file.
    """
    info_list = []

    # Loop through all JSON files in the annotations folder.
    for filename in os.listdir(annotations_folder):
        if filename.lower().endswith(".json"):
            print("Processing file:", filename)
            file_path = os.path.join(annotations_folder, filename)
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
            except Exception as e:
                print(f"Error loading {filename}: {e}")
                continue

            # Get and adjust the image path
            original_image_path = data.get("image_path", "")
            if original_image_path.startswith("DATASET/images/"):
                image_path = os.path.join("/home/asareen/shared/Prognosis/Kolhapur_Sentinel_Images/images/",
                                          original_image_path[len("DATASET/images/"):])
            else:
                image_path = original_image_path

            epi_week = data.get("epiweeks", "")
            cases = data.get("dynamic", {}).get("cases", {}).get("dengue_cases", None)
            precipitation = data.get("dynamic", {}).get("environmental_data", {}).get("precipitation", None)
            temperature = data.get("dynamic", {}).get("environmental_data", {}).get("temperature", None)

            # Derive LastDayWeek from the filename, e.g., "image_2016-01-03.json" → "2016-01-03"
            last_day_week = filename.replace("image_", "").replace(".json", "")

            info_list.append({
                "LastDayWeek": last_day_week,
                "cases_medellin": cases,
                "precipitation": precipitation,
                "temperature": temperature,
                "epi_week": epi_week,
                "image_path": image_path
            })

    out_df = pd.DataFrame(info_list)
    out_df.to_csv(output_csv, index=False)
    print(f"CSV file created: {output_csv}")
    return out_df

def combineData(img_folder, df, resize_ratio=None):
    """
    Combines the CSV and image files by matching the date in the CSV’s
    'LastDayWeek' field to the date derived from each image filename.
    Prints out which rows or image files could not be processed.
    """
    info_dict = {'LastDayWeek':[], 'cases_medellin':[], 'Image':[], 'epi_week':[]}
    img_list = os.listdir(img_folder)

    for index, row in df.iterrows():
        name = row['LastDayWeek']
        try:
            week_df = str(getEpiWeek(name))
        except Exception as e:
            print("Error processing date for row index", index, "with LastDayWeek:", name, "Error:", e)
            continue

        case = row['cases_medellin']
        found_image = False
        for img_name in img_list:
            new_img_name = ''.join(i for i in img_name if i.isdigit() or i == '-')
            try:
                week_img = str(getEpiWeek(new_img_name))
            except Exception as e:
                print("Error processing image filename:", img_name, "Error:", e)
                continue

            if week_df == week_img:
                img_path = os.path.join(img_folder, img_name)
                try:
                    img = readImg(img_path, resize_ratio)
                except Exception as e:
                    print("Error reading image:", img_path, "Error:", e)
                    continue
                info_dict['Image'].append(img)
                info_dict['LastDayWeek'].append(name)
                info_dict['cases_medellin'].append(case)
                info_dict['epi_week'].append(week_df)
                found_image = True
                break
        if not found_image:
            print("No matching image found for row index", index, "with LastDayWeek:", name)
    return info_dict

# Example usage:
# First, create the CSV from the annotation JSONs:
annotations_folder = "/home/asareen/shared/Prognosis/Kolhapur_Sentinel_Images/annotations/530"
output_csv = "/home/asareen/shared/Prognosis/Kolhapur_Sentinel_Images/combined_data.csv"
combined_df = create_csv_from_jsons(annotations_folder, output_csv)
print(combined_df.head())

# Now, load your CSV and images:
csv_file = output_csv  # This CSV was created above.
img_folder = "/home/asareen/shared/Prognosis/Kolhapur_Sentinel_Images/images/530/" # This is your dataset_rbg (images)
df = pd.read_csv(csv_file)
info_dict = combineData(img_folder, df, resize_ratio=(0.7, 0.7, 1))

print('--------------------INFO_DICT---------------------')
print('keys:', info_dict.keys())
print('')

print('-------------------DENGUE CASES-------------------')
print('Max weekly dengue cases:', max(info_dict['cases_medellin']))
print('Min weekly dengue cases:', min(info_dict['cases_medellin']))
print('')

print('----------------------WEEKS-----------------------')
print('Max week:', max(info_dict['LastDayWeek']))
print('Min week:', min(info_dict['LastDayWeek']))

import torch.nn as nn
import torch.optim as optim
train_val_ratio = 0.7
train_num = int(len(info_dict['Image']) * train_val_ratio)

  # Change list to array
origin_dimension_X = np.array(info_dict['Image'])
labels = np.array(info_dict['cases_medellin'])

print(''.center(60,'-'))

origin_X_train = origin_dimension_X[:train_num,:,:,:]
y_train = labels[:train_num]
origin_X_test = origin_dimension_X[train_num:,:,:,:]
y_test = labels[train_num:]

print(f"origin_X_train: {origin_X_train.shape}")
print(f"y_train: {y_train.shape}")
print(f"origin_X_train: {origin_X_test.shape}")
print(f"y_train: {y_test.shape}")

def dimension_reduct_with_PCA(origin_X_train, origin_X_test, y_train):
  print(' PRINCIPAL COMPONENT ANALYSIS  '.center(100, '='))

  reshape_X_train = origin_X_train.reshape(origin_X_train.shape[0], -1)
  reshape_X_test = origin_X_test.reshape(origin_X_test.shape[0], -1)

  pca = PCA(n_components=0.95)
  pca_X_train = pca.fit_transform(reshape_X_train)

  pca_X_test = pca.transform(reshape_X_test)
  print('Origin shape'.ljust(15), reshape_X_train.shape)
  print('Resize shape'.ljust(15), pca_X_train.shape)

  return pca_X_train, pca_X_test

origin_X_train_PCA, origin_X_test_PCA = dimension_reduct_with_PCA(origin_X_train, origin_X_test, y_train)
print(f"origin_X_train: {origin_X_train_PCA.shape}")
print(f"y_train: {y_train.shape}")
print(f"origin_X_train: {origin_X_test_PCA.shape}")
print(f"y_train: {y_test.shape}")

###########################################
#  Next Steps: Model Training and Evaluation
###########################################

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score
from torch.utils.data import DataLoader, Dataset

train_val_ratio = 0.8
train_num = int(len(info_dict['Image']) * train_val_ratio)

  # Change list to array
origin_dimension_X = np.array(info_dict['Image'])
labels = np.array(info_dict['cases_medellin'])

print(''.center(60,'-'))

origin_X_train = origin_dimension_X[:train_num,:,:,:]
y_train = labels[:train_num]
origin_X_test = origin_dimension_X[train_num:,:,:,:]
y_test = labels[train_num:]

print(f"origin_X_train: {origin_X_train.shape}")
print(f"y_train: {y_train.shape}")
print(f"origin_X_train: {origin_X_test.shape}")
print(f"y_train: {y_test.shape}")


origin_X_train_PCA, origin_X_test_PCA = dimension_reduct_with_PCA(origin_X_train, origin_X_test, y_train)
print(f"origin_X_train: {origin_X_train_PCA.shape}")
print(f"y_train: {y_train.shape}")
print(f"origin_X_train: {origin_X_test_PCA.shape}")
print(f"y_train: {y_test.shape}")



from torch.utils.data import Dataset, DataLoader

class TrainDataset(Dataset):
    def __init__(self, data, y):
        self.data = data
        self.y = y
    def __len__(self):
        return self.data.shape[0]
    def __getitem__(self, ind):
        x = self.data[ind]
        y = self.y[ind]
        return x, y
    
class TestDataset(TrainDataset):
    def __getitem__(self, ind):
        x = self.data[ind]
        return x
    
train_set = TrainDataset(origin_X_train_PCA, y_train)
test_set  = TrainDataset(origin_X_test_PCA, y_test)

batch_size = 1
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)



input_size = origin_X_train_PCA.shape[1]
print(input_size)
hidden_size = 230
num_layers = 2
num_classes = 1
output_size = 1
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#model.double()

class flightLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.fc_1 = nn.Linear(hidden_size,128)
        self.relu = nn.ReLU()
        self.fc   = nn.Linear(128, output_size)   
        
    def forward(self, x, hs, cs):
      
        out, (hs,cs) = self.lstm(x, (hs,cs)) # out.shape = (batch_size, seq_len, hidden_size)
        # output, (hn, cn) = self.lstm(x, (h_0, c_0))
        out = out.view(-1, self.hidden_size) # out.shape = (seq_len, hidden_size)     
        out = self.fc_1(out)
        out = self.relu(out)
        out = self.fc(out)       
        return out

x = torch.randn((1, 1000, 13)) # batch - seq length - hidden size
hs = torch.zeros(2, x.size(0), hidden_size)
cs = torch.zeros(2, x.size(0), hidden_size)
model = flightLSTM(input_size, hidden_size, num_layers, num_classes)
pred = model(x, hs, cs)
print(pred.shape)


model = flightLSTM(input_size, hidden_size, num_layers, output_size)
model.to(device)


#from model import MPL_model

# https://medium.com/deep-learning-study-notes/multi-layer-perceptron-mlp-in-pytorch-21ea46d50e62
# cnn https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/


import torch
import torch.nn as nn
# PyTorch libraries and modules
import torch
from torch.autograd import Variable
from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout
from torch.optim import Adam, SGD

criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

epochs = 100
running_mae = 0

losses = []

v_losses = []
# MAE
maes = []
mae_list = []
v_maes = []
v_mae_list = []
# MAPE
mape = []
mapes = []
mape_val = []
mapes_val = []



for epoch in range(epochs):

    for batch_num, input_data in enumerate(train_loader):
        model.train()
        optimizer.zero_grad()
        x, y = input_data
        x = x.to(device).float()
        y = y.to(device)
        x = x.unsqueeze(0)

        hs = torch.zeros(2, x.size(0), hidden_size).to(device)  
        cs = torch.zeros(2, x.size(0), hidden_size).to(device)

        output = model(x,hs,cs)
        

        loss = criterion(output, y.float())
        loss.backward()
        losses.append(loss.item())
        
        optimizer.step()

        # MAE
        error = torch.abs(output - y).sum().data
        maes.append(error)

        # MAPE 
        mape = torch.abs((output-y)/output).sum().data
        mapes.append(mape)

    with torch.no_grad():
        for batch_num, input_data in enumerate(test_loader):
            model.eval()
            optimizer.zero_grad()
            x, y = input_data
            x = x.to(device).float()
            y = y.to(device)
            x = x.unsqueeze(0)

            hs = torch.zeros(2, x.size(0), hidden_size).to(device)  
            cs = torch.zeros(2, x.size(0), hidden_size).to(device)

            output = model(x,hs,cs)

            v_loss = criterion(output, y.float())
            
            v_losses.append(loss.item())
            
            
            # MAE
            v_error = torch.abs(output - y).sum().data
            v_maes.append(v_error)

            # MAPE 
            
            mape_val = torch.abs((output-y)/output).sum().data
            mapes_val.append(mape_val)
        
    
    # MAE
    mae_list.append(sum(maes)/len(maes))
    v_mae_list.append(sum(v_maes)/len(v_maes))
    print('Epoch %d | Loss_training %6.2f | MAE_training %6.2f | MAPE_training %6.2f' % (epoch, sum(losses)/len(losses), sum(maes)/len(maes), 
                                                                                         sum(mapes)/len(mapes)))
    print('Epoch %d |      val_Loss %6.2f |      val_MAE %6.2f |      MAPE_val %6.2f' % (epoch, sum(v_losses)/len(v_losses), sum(v_maes)/len(v_maes),
                                                                                         sum(mapes_val)/len(mapes_val)))


    epochs = range(0,100)
plt.figure(figsize=[8., 6.])
plt.plot(epochs, mae_list,  label='MAE')
plt.plot(epochs, v_mae_list, label='Val_MAE')
plt.title('Training MAE vs epochs')
plt.xlabel('Epochs')
plt.show()



model_save_path = "/home/asareen/shared/Prognosis/saved_models/lstm_model.pth"
torch.save(model.state_dict(), model_save_path)
print(f"Model saved to {model_save_path}")
